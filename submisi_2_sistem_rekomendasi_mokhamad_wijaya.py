# -*- coding: utf-8 -*-
"""Submisi 2 Sistem Rekomendasi - Mokhamad Wijaya.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PumAvYSjUXpIu3vB6PMTexfkFmgV7BjQ

# Data Loading
"""

!wget -c "https://storage.googleapis.com/kaggle-data-sets/1004280/5624361/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20230730%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20230730T143250Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=1b331f202b13c1f98a75982d31bc5d8ae7a81c2ff184ada587d42ad6a58c69b420f763a202e6c2280548c9284f9c02e453b92ea858d49ff1154018a075424a6bf34360cde644ed6d3ba4d2fba29a0d4a4a9a73fcb924b67c3c57180e98b72440d310cd79d6f20338d7b480363652b01ad88545f4109af37a9fe157cb0a70ef3f18038a1109424d46321b96252593a0c470d60ed0dd368be18588483371c819069d660952fdb0473d7ba3fb4510ce52f9191c72b378da3c626ac2d44efe37b400ce4f7910c202ad8f3425574c6fce2efced7b590817370ea5d184168caa9b19faaa13fe446c47073b853f6a8f07042b6a80a023d8c76c73226149c99910c06595" \
  -O "archive.zip"

!unzip archive.zip

import pandas as pd
import numpy as np

ds_books = pd.read_csv('Books.csv')
ds_ratings = pd.read_csv('Ratings.csv')
ds_users = pd.read_csv('Users.csv')

"""# Exploratory Data Analysis

## Data Understanding

### Kondisi Data
"""

print(
    f"Dataset books: jumlah data {ds_books.shape[0]} dengan {ds_books.shape[1]} kolom"
    + f"\nyang berisikan {list(ds_books.columns)}")
print(
    f"Dataset ratings: jumlah data {ds_ratings.shape[0]} dengan {ds_ratings.shape[1]} kolom"
    + f"\nyang berisikan {list(ds_ratings.columns)}")
print(
    f"Dataset users: jumlah data {ds_users.shape[0]} dengan {ds_users.shape[1]} kolom"
    + f"\nyang berisikan {list(ds_users.columns)}")

ds_books.head()

ds_ratings.head()

ds_users.head()

"""### Tipe Data"""

print("Tipe data pada dataset Books")
print(ds_books.info())
print("Tipe data pada dataset Ratings")
print(ds_ratings.info())
print("Tipe data pada dataset Users")
print(ds_users.info())

"""### Periksa Duplikasi Data"""

dpl_books = ds_books[ds_books.duplicated()]
print("jumlah baris duplikasi dataset Books: ", dpl_books.shape[0])
dpl_ratings = ds_ratings[ds_ratings.duplicated()]
print("jumlah baris duplikasi dataset Ratings: ", dpl_ratings.shape[0])
dpl_users = ds_users[ds_users.duplicated()]
print("jumlah baris duplikasi dataset Users: ", dpl_users.shape[0])

"""### Periksa Missing value Dataset"""

ds_books.isnull().sum()

ds_ratings.isnull().sum()

ds_users.isnull().sum()

"""### Periksa data unik Dataset"""

for fitur in ds_books.columns:
  print(f"\nData unik fitur {fitur}: ")
  print(ds_books[fitur].unique())

for fitur in ds_ratings.columns:
  print(f"\nData unik fitur {fitur}: ")
  print(ds_ratings[fitur].unique())

for fitur in ds_users.columns:
  print(f"\nData unik fitur {fitur}: ")
  print(ds_users[fitur].unique())

"""## Data Preprocessing

Untuk dataset Books, dikarenakan kolom `Year-Of-Publication` seharusnya bertipe data numerik karena berkaitan dengan waktu, maka akan diubah tipe data tersebut menjadi numerik
"""

try:
  ds_books['Year-Of-Publication'] = ds_books['Year-Of-Publication'].astype('int64')
except Exception as e:
  print(e)

"""terdapat tahun publikasi yang terisi bukan dengan tahun melainkan dengan teks, sehingga perlu penangan agar kolom `Year-Of-Publication` berisikan tipe data numerik"""

non_numerik_year_idx = []
for i in range(ds_books.shape[0]):
  try:
    int(ds_books.loc[i, 'Year-Of-Publication'])
  except:
    non_numerik_year_idx.append(np.int64(i))
    print(i)
    # print(ds_books.loc[i, 'Year-Of-Publication'])

"""Terdapat 3 data yang tahun publikasinya tidak bisa diubah ke tipe data numerik"""

ds_books.iloc[non_numerik_year_idx]

"""dikarenakan terjadi kesalahan data antara kolom `Book-Author` dan `Year-of-Publication` maka dilakukan pertukaran `value` pada kedua kolom tersebut agar sesuai."""

for idx in non_numerik_year_idx:
  year = ds_books.iloc[idx]['Book-Author']
  author = ds_books.iloc[idx]['Year-Of-Publication']
  ds_books.iloc[idx]['Book-Author'] = author
  ds_books.iloc[idx]['Year-Of-Publication'] = year
ds_books.iloc[non_numerik_year_idx]

"""Kemudian diubah nilai kolom `Year-Of-Publication` ke tipe data numerik"""

ds_books['Year-Of-Publication'] = ds_books['Year-Of-Publication'].astype('int64')

ds_books.info()

ds_books_rating = pd.merge(ds_books, ds_ratings, on='ISBN')
ds_books_rating.head()

ds_books_rating.isnull().sum()

"""Setelah apakah terdapat _missing value_ pada `Dataframe` yang merupakan gabungan antara dataset Books dan Ratings, dengan mengecualikan kolom `Image-URL-L`, maka total data yang memiliki _missing value_ berjumlah 3 sehingga dapat dilakukan _drop_ pada dara tersebut"""

ds_books_rating.dropna(inplace=True)
ds_books_rating.isnull().sum()

"""Sekarang dataset sudah bersih dari _missing value_ sehingga dapat dilakukan proses selanjutnya yaitu menggabungkan dataset tersebut dengan dataset Users."""

df = pd.merge(ds_books_rating, ds_users, on='User-ID')
df.head()

df.isnull().sum()

df.isna().sum()

df.drop_duplicates(subset=["Book-Title"], inplace=True)

"""Selanjutnya adalah mengubah tipe data non numerik menjadi numerik."""

df.info()

"""Untuk mempersingkat waktu pelatihan, maka akan dilakukan data sampling pada 9000 data saja"""

df = df.sample(n=5000, random_state=3)

"""### Data Encoding"""

from sklearn.preprocessing import LabelEncoder

kolom_obyek = ['ISBN', 'Book-Title', 'Book-Author', 'Publisher', 'Location', 'User-ID']
label_encoders = {
    'ISBN': LabelEncoder(),
    'Book-Title': LabelEncoder(),
    'Book-Author': LabelEncoder(),
    'Publisher': LabelEncoder(),
    'Location': LabelEncoder(),
    'User-ID': LabelEncoder()}
df_encoded = df.copy()
for col in kolom_obyek:
    df_encoded[col] = label_encoders[col].fit_transform(df_encoded[col])
df_encoded.head()

# Mengubah User-ID menjadi list tanpa nilai yang sama
user_ids = df['User-ID'].unique().tolist()
print('list User-ID: ', user_ids)

# Melakukan encoding User-ID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded User-ID : ', user_to_user_encoded)

# Melakukan proses encoding angka ke ke User-ID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke User-ID: ', user_encoded_to_user)

# Mengubah ISBN menjadi list tanpa nilai yang sama
isbn_ids = df['ISBN'].unique().tolist()

# Melakukan proses encoding ISBN
isbn_to_isbn_encoded = {x: i for i, x in enumerate(isbn_ids)}

# Melakukan proses encoding angka ke ISBN
isbn_encoded_to_isbn = {i: x for i, x in enumerate(isbn_ids)}
print('encoded angka ke ISBN: ', user_encoded_to_user)

# Mapping User-ID ke dataframe user
df['user'] = df['User-ID'].map(user_to_user_encoded)

# Mapping ISBN ke dataframe isbn
df['isbn'] = df['ISBN'].map(isbn_to_isbn_encoded)

#  Mengubah rating menjadi nilai float
df['rating'] = df['Book-Rating'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(df['rating'])
# Nilai maksimal rating
max_rating = max(df['rating'])
# Mendapatkan jumlah ISBN
num_isbn = len(isbn_to_isbn_encoded)
# num_isbn = len(label_encoders['ISBN'].classes_)
# Mendapatkan jumlah User-ID
num_users = len(user_to_user_encoded)
# num_users = len(label_encoders['User-ID'].classes_)

print('Number of User: {}, Number of Resto: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_isbn, min_rating, max_rating
))

"""#### Train-Test Split"""

# Membuat variabel x untuk mencocokkan data user dan book menjadi satu value
# x = df_encoded[['User-ID', 'ISBN']].values
x = df[['user', 'isbn']].values

# Membuat variable y untuk menampung data rating buku
y = df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values
# Membagi menjadi 90% data train dan 10% data validasi
train_indices = int(0.9 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

x_train.shape[0], x_val.shape[0]

"""# Model Development

## Collaborative Filtering

### Model RecommenderNet
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import Flatten,Embedding,Dense,Concatenate

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_isbn, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_isbn = num_isbn
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.isbn_embedding = layers.Embedding( # layer embeddings isbn
        num_isbn,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.isbn_bias = layers.Embedding(num_isbn, 1) # layer embedding isbn bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    isbn_vector = self.isbn_embedding(inputs[:, 1]) # memanggil layer embedding 3
    isbn_bias = self.isbn_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_isbn = tf.tensordot(user_vector, isbn_vector, 2)

    x = dot_user_isbn + user_bias + isbn_bias

    return tf.nn.sigmoid(x) # activation sigmoid

# Inisialisasi Model
model = RecommenderNet(num_users, num_isbn, 50)

# Model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Memulai training
history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 100,
    validation_data = (x_val, y_val)
)

model.summary()

"""### Evaluasi Model"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# Evaluasi nilai RMSE model
_, rmse = model.evaluate(x_val, y_val)
print('RMSE:', rmse)

"""# Inference Model untuk memberikan rekomendasi buku"""

# Mengambil sample user
user_id = df['User-ID'].sample(1).iloc[0]
isbn_rated_by_user = df[df['User-ID'] == user_id]
print(user_id)

isbn_rated_by_user

isbn_not_rated = ds_books[~ds_books['ISBN'].isin(isbn_rated_by_user['ISBN'].values)]['ISBN']
isbn_not_rated = list(
    set(isbn_not_rated)
    .intersection(set(isbn_to_isbn_encoded.keys()))
)

isbn_not_rated = [[isbn_to_isbn_encoded.get(x)] for x in isbn_not_rated]
user_encoder = user_to_user_encoded.get(user_id)
user_isbn_array = np.hstack(
    ([[user_encoder]] * len(isbn_not_rated), isbn_not_rated)
)

ratings = model.predict(user_isbn_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_isbn_ids = [
    isbn_encoded_to_isbn.get(isbn_not_rated[x][0]) for x in top_ratings_indices
]

print('Menampilkan daftar rekomendasi untuk pengguna dengan id: {}'.format(user_id))
print('===' * 9)
print('Buku dengan rating yang tinggi dari pengguna')
print('----' * 8)

top_isbn_user = (
    isbn_rated_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .ISBN.values
)

isbn_df_rows = ds_books[ds_books['ISBN'].isin(top_isbn_user)]
for row in isbn_df_rows.itertuples():
    print(f"{row.ISBN} - Judul: {row._2} - Author: {row._3}")

print('----' * 8)
print('Top 10 Book recommendation')
print('----' * 8)

recommended_isbn = ds_books[ds_books['ISBN'].isin(recommended_isbn_ids)]
for row in recommended_isbn.itertuples():
    print(f"{row.ISBN} - Judul: {row._2} Author: {row._3}")

